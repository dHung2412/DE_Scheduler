# Data Pipeline for GitHub Events

Dá»± Ã¡n nÃ y triá»ƒn khai má»™t Data Pipeline toÃ n diá»‡n (End-to-End), Ä‘Æ°á»£c thiáº¿t ká»ƒ Ä‘á»ƒ xá»­ lÃ½ dá»¯ liá»‡u sá»± kiá»‡n thá»i gian thá»±c tá»« GitHub, sá»­ dá»¥ng kiáº¿n trÃºc Lakehouse hiá»‡n Ä‘áº¡i.

## ğŸ›  Tech Stack
*   **Ingestion**: FastAPI & Kafka
*   **Orchestration**: [Apache Airflow](https://airflow.apache.org/)
*   **Processing Engine**: [Apache Spark](https://spark.apache.org/) (Streaming & Batch)
*   **Table Format**: [Apache Iceberg](https://iceberg.apache.org/)
*   **Transformation**: [dbt](https://www.getdbt.com/)
*   **Storage**: [MinIO](https://min.io/) (S3 Compatible)
*   **Infrastructure**: Docker & Docker Compose

## ğŸ“‚ Project Structure
- `airflow/`: Cáº¥u hÃ¬nh Docker build vÃ  Plugins cho Airflow.
- `dags/`: Chá»©a cÃ¡c pipeline Ä‘iá»u phá»‘i (DAGs) vÃ  code Spark.
- `dbt_project/`: Project dbt quáº£n lÃ½ logic transform dá»¯ liá»‡u (Silver -> Gold).
- `metric_collector/`: Service API nháº­n dá»¯ liá»‡u vÃ  Kafka Producer.
- `docker-compose.yaml`: Äá»‹nh nghÄ©a toÃ n bá»™ háº¡ táº§ng (Infrastructure as Code).

## ğŸ— Kiáº¿n trÃºc há»‡ thá»‘ng (Architecture)
Há»‡ thá»‘ng sá»­ dá»¥ng kiáº¿n trÃºc **Lambda Architecture** giáº£n lÆ°á»£c, káº¿t há»£p giá»¯a Streaming (Ingestion) vÃ  Batch (Processing).

![Pipeline Architecture](./utils/pipeline.png)

### Detailed Pipeline Flow

**1. Ingestion Layer: API & Kafka (High Throughput & Reliability)**
*   **API Gateway**: Thiáº¿t káº¿ theo hÆ°á»›ng **Non-blocking I/O** & **Fail-Fast**.
    *   Nháº­n payload JSON -> GÃ¡n Trace ID (`event_id`) -> Äáº©y vÃ o Memory Queue (`put_nowait`).
    *   Pháº£n há»“i client tá»©c thÃ¬ vá»›i Ä‘á»™ trá»… cá»±c tháº¥p (<10ms).
    *   CÆ¡ cháº¿ **Backpressure**: Tráº£ vá» `503 Service Unavailable` khi hÃ ng Ä‘á»£i Ä‘áº§y Ä‘á»ƒ báº£o vá»‡ tÃ i nguyÃªn server.
*   **Producer Worker**:
    *   **Hybrid Batching Strategy**: Tá»± Ä‘á»™ng chuyá»ƒn Ä‘á»•i giá»¯a 'Polling' (High Load) vÃ  'Waiting' (Low Latency) Ä‘á»ƒ tá»‘i Æ°u throughput.
    *   **Non-blocking Serialization**: DÃ¹ng `run_in_executor` Ä‘á»ƒ Ä‘áº©y tÃ¡c vá»¥ nÃ©n Avro (CPU-bound) sang thread riÃªng, giá»¯ cho Event Loop luÃ´n mÆ°á»£t mÃ .
    *   **Data Durability**: CÆ¡ cháº¿ Retry káº¿t há»£p **Local Fallback** giÃºp Ä‘áº£m báº£o khÃ´ng máº¥t mÃ¡t dá»¯ liá»‡u (Zero Data Loss) ká»ƒ cáº£ khi Kafka gáº·p sá»± cá»‘.

**2. Bronze Layer: Streaming Ingestion (Kafka -> Iceberg)**
*   Spark Structured Streaming Ä‘á»c liÃªn tá»¥c tá»« Kafka topic.
*   Sá»­ dá»¥ng **UDF Decoder** giáº£i mÃ£ Avro binary ngay trong Spark.
*   **Flattening**: LÃ m pháº³ng cáº¥u trÃºc JSON lá»“ng nhau.
*   Ghi vÃ o báº£ng Iceberg `demo.bronze.github_events` vá»›i cháº¿ Ä‘á»™ Fanout Writer.

**3. Silver Layer (Part 1): Raw to Structured (Spark Batch)**
*   Trigger bá»Ÿi Airflow Ä‘á»‹nh ká»³ (Hourly).
*   **Incremental Load**: Chá»‰ Ä‘á»c dá»¯ liá»‡u má»›i tá»« Bronze dá»±a vÃ o watermark `ingestion_timestamp`.
*   **Parsing**: Parse cá»™t `payload` JSON thÃ nh cÃ¡c cá»™t quan trá»ng (PR details, Issue state...).
*   **Upsert**: Sá»­ dá»¥ng `MERGE INTO` Ä‘á»ƒ deduplicate dá»¯ liá»‡u.

**4. Silver Layer (Part 2): Structured to Enriched (dbt)**
*   LÃ m sáº¡ch dá»¯ liá»‡u, chuáº©n hÃ³a Ä‘á»‹nh dáº¡ng chuá»—i.
*   **Event Categorization**: PhÃ¢n loáº¡i sá»± kiá»‡n (Code Change, Social, Management...).
*   TÃ­nh toÃ¡n Activity Score cho tá»«ng event.

**5. Gold Layer: Aggregation & Business Insights (dbt)**
*   **Daily Aggregation**: Tá»•ng há»£p hoáº¡t Ä‘á»™ng ngÆ°á»i dÃ¹ng theo ngÃ y.
*   **User Profiling**: PhÃ¢n loáº¡i User (Developer, Reviewer, etc.) dá»±a trÃªn hÃ nh vi Ä‘Ã³ng gÃ³p.
*   TÃ­nh toÃ¡n cÃ¡c chá»‰ sá»‘ xu hÆ°á»›ng (Rolling Average).

**6. Orchestration**
*   **Airflow DAG** cháº¡y Ä‘á»‹nh ká»³ má»—i giá»:
    1.  `Spark Job`: Bronze -> Silver Parsed.
    2.  `dbt run`: Silver Enriched update.
    3.  `dbt run`: Gold User Activity update.
    4.  `dbt test`: Kiá»ƒm tra cháº¥t lÆ°á»£ng dá»¯ liá»‡u (Unique ID, Not Null...).

**7. Maintenance Layer (Iceberg Table Optimization)**
Há»‡ thá»‘ng Iceberg cáº§n Ä‘Æ°á»£c báº£o trÃ¬ Ä‘á»‹nh ká»³ Ä‘á»ƒ giáº£i quyáº¿t váº¥n Ä‘á» "Small Files" do quÃ¡ trÃ¬nh Streaming sinh ra liÃªn tá»¥c.
*   **Daily Job**: ([af_maintenance_bronze_by_day.py])
    *   Thá»±c hiá»‡n **Rewrite Data Files (Compaction)** sá»­ dá»¥ng chiáº¿n thuáº­t Bin-pack.
    *   Gom hÃ ng ngÃ n file nhá» (KB) thÃ nh cÃ¡c file tiÃªu chuáº©n (~20MB) Ä‘á»ƒ tÄƒng tá»‘c Ä‘á»™ Ä‘á»c cho Spark/dbt.
*   **Weekly Job**: ([af_maintenance_bronze_by_week.py])
    *   **Expire Snapshots**: XÃ³a cÃ¡c version dá»¯ liá»‡u cÅ© (Time Travel) khÃ´ng cÃ²n cáº§n thiáº¿t.
    *   **Cleanup**: XÃ³a bá» cÃ¡c file rÃ¡c (Orphan files) vÃ  Manifest cÅ© Ä‘á»ƒ giáº£i phÃ³ng dung lÆ°á»£ng Storage.

## ğŸš€ Getting Started

### 1. Prerequisites
- Docker & Docker Compose installed.
- RAM tá»‘i thiá»ƒu: 6-8GB.

### 2. Setup Environment
Táº¡o file [.env] táº¡i thÆ° má»¥c gá»‘c vÃ  cáº¥u hÃ¬nh cÃ¡c thÃ´ng sá»‘ káº¿t ná»‘i (tham kháº£o file `docker-compose.yaml`):
